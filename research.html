<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Research | Minghao Liu</title>
  <style>
    body {
      font-family: sans-serif;
      line-height: 1.6;
      max-width: 900px;
      margin: 0 auto;
      padding: 20px;
      background: #fdfdfd;
    }
  </style>
</head>
<body>

  <nav style="text-align: center; margin-bottom: 20px;">
    <a href="index.html">Home</a> |
    <a href="publications.html">Publications</a> |
    <a href="research.html">Research</a> |
    <a href="activities.html">Activities</a>
  </nav>

  <h2>Research Experience</h2>
  <ul>
    <li><strong>UROP, HKUST</strong> — Advisor: Dan Xu (Jun–Aug 2023)
      <ul>
        <li>Depth estimation using diffusion models with UNet-based architecture.</li>
        <li>Designed interpolation algorithm for NYU-Depth V2 dataset.</li>
      </ul>
    </li>
    <li><strong>UROP, HKUST</strong> — Advisor: Yu Hu (Sep–Dec 2023)
      <ul>
        <li>Studied Firing Rate Network Model on zebrafish neural data.</li>
        <li>Applied PINNs and statistical learning to brain-wide simulation.</li>
      </ul>
    </li>
    <li><strong>KnowComp Group, HKUST</strong> — Advisor: Yangqiu Song (Feb–Sep 2024)
      <ul>
        <li><strong>BrainASER</strong>: Analyzing alignment between fMRI brain activity and knowledge graphs.</li>
        <li><strong>IntentionQA</strong>: Built benchmark to test purchase intent comprehension in LMs.</li>
      </ul>
    </li>
    <li><strong>RenLab, HKUST</strong> — Advisor: <a href="https://mayrfung.github.io/" target="_blank">Yi R. (May) Fung</a> (Feb 2025 – Present)
      <ul>
        <li>Exploring interpretability in multimodal LLMs (e.g., LLaVA), focusing on hallucinations.</li>
        <li>Developing evaluation for medical image editing and multimodal outputs.</li>
      </ul>
    </li>
  </ul>

</body>
</html>
